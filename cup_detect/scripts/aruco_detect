#!/usr/bin/env python
import cv2
import numpy as np
import rospy
from tf import transformations 
from cv_bridge import CvBridge, CvBridgeError
from sensor_msgs.msg import Image
from geometry_msgs.msg import Pose

class ArucoDetect:
  def __init__(self, robot_id, cup_id):

    #create cv bridge instance
    self.cvBridge = CvBridge()

    #create  camera_subscriber
    self.image_sub = rospy.Subscriber('/usb_cam/image_raw', Image, self.image_callback)
    
    #create pose pub
    self.cup_pose_pub = rospy.Publisher('pose_data', Pose, queue_size=1)

    #create bounding box image pub
    self.bounding_image_pub = rospy.Publisher('/bounding_image', Image, queue_size=1)

    #load aruco marker dictionary
    self.aruco_dict = cv2.aruco.Dictionary_get(cv2.aruco.DICT_6X6_250)

    #create aruco params
    self.aruco_param = cv2.aruco.DetectorParameters_create()
    self.cam_matrix = np.array([[ 691.64109,    0.,          305.03735],[   0.,          695.32384,  296.75875],[   0.,            0.,            1.,        ]])
    self.dist = np.array([[ 0.24028,   -0.68609,   0.01619,   -0.00456,  0.00000]])
    self.cup_id = cup_id
    self.robot_id = robot_id


  def image_callback(self, msg):
    # convert image to cv
    try:
      cv_image = self.cvBridge.imgmsg_to_cv2(msg)
    except CvBridgeError:
      print('fail to convert to cv2 image')


    # convert image to graysecale
    gs_image = cv2.cvtColor(cv_image, cv2.COLOR_RGB2GRAY)
    
    # detect markers in vision
    bounding_boxes, ids, rejected_markers = cv2.aruco.detectMarkers(gs_image, self.aruco_dict, parameters=self.aruco_param)
    
    # publish image with detected markers for debugging
    cv2.aruco.drawDetectedMarkers(gs_image, bounding_boxes)
    img_msg = self.cvBridge.cv2_to_imgmsg(gs_image)
    self.bounding_image_pub.publish(img_msg)

    # check if the required AR Tags can be seen
    if (len(np.where(ids == self.cup_id)[0]) == 1) and (len(np.where(ids == self.robot_id)[0]) == 1):

      # get index value of robot
      index_of_robot = np.where(ids==self.robot_id)[0][0]

      #get index value of cup
      index_of_cup = np.where(ids==self.cup_id)[0][0]

      # limit to important bounding boxes
      important_bounding_boxes = [bounding_boxes[index_of_cup], bounding_boxes[index_of_robot]]

      # Determine marker poses
      [rvecs, tvecs] = cv2.aruco.estimatePoseSingleMarkers(important_bounding_boxes, 6, self.cam_matrix, self.dist)

      #TODO check that rotation and translation values are being set to correct axis
      # Create cup transform matrix
      cup_transform = transformations.quaternion_matrix(transformations.quaternion_from_euler(rvecs[0][0][0], rvecs[0][0][1], rvecs[0][0][2]))
      cup_transform[0][3] = tvecs[0][0][0]
      cup_transform[1][3] = tvecs[0][0][1]
      cup_transform[2][3] = tvecs[0][0][2]

      #TODO check that rotation and translation values are being set to correct axis
      # Create cup transform matrix
      robot_transform = transformations.quaternion_matrix(transformations.quaternion_from_euler(rvecs[0][0][0], rvecs[0][0][1], rvecs[0][0][2]))
      robot_transform[0][3] = tvecs[1][0][0]
      robot_transform[1][3] = tvecs[1][0][1]
      robot_transform[2][3] = tvecs[1][0][2]

      #get relative transform
      relative_transform = np.matmul(np.linalg.inv(robot_transform),cup_transform)

      #get relative quaternion
      relative_q = transformations.quaternion_from_matrix(relative_transform)

      #get relative pose of robot base to cup
      pose_msg = Pose()
      pose_msg.orientation.x = relative_q[0]
      pose_msg.orientation.y = relative_q[1]
      pose_msg.orientation.z = relative_q[2]
      pose_msg.orientation.w = relative_q[3]
      pose_msg.position.x = relative_transform[0][3]
      pose_msg.position.y = relative_transform[1][3]
      pose_msg.position.z = relative_transform[2][3]
      #publish cup pose
      self.cup_pose_pub.publish(pose_msg)


if __name__ == '__main__':
  rospy.init_node('cup_detector')
  cupDetector = ArucoDetect(5,7)

  while not rospy.is_shutdown():
    rospy.spin()

